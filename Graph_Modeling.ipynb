{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph_Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kmgN5oJGLhLT",
        "riUQd_gG9PRY",
        "Rzo66rzzAdME",
        "UZo0DJkiAhpb",
        "MjptK0FbKAxd",
        "Do6jyX4VOIMT",
        "dpqPM4awUBAA",
        "pALKUNIP_lMJ",
        "lwbBkhMFCCV2",
        "RnGIUSfqCFF3",
        "h6dlbbZlkTcy",
        "3T3u_MuyCuDO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmgN5oJGLhLT"
      },
      "source": [
        "# Eternal Module Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FjoSWK_TT4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2d485e-f5f8-4c54-88b9-127909f8e0e2"
      },
      "source": [
        "! pip install openbiolink"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openbiolink\n",
            "  Downloading openbiolink-0.1.4-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 15.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from openbiolink) (4.64.0)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.12.1+cu113)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from openbiolink) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->openbiolink) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->openbiolink) (4.1.1)\n",
            "Installing collected packages: openbiolink\n",
            "Successfully installed openbiolink-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WTGOZUO7ggB",
        "outputId": "b2e61316-de5f-40ea-e824-3c1bd76ee389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 14.2 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 325 kB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 60.2 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=2faf5516dca61b9fc99210b0ec743fa4ba0ca0e06e128786e060bc2e2d983b82\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "# !pip install torch-geometric"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5pkzL52Y66Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "# !pip install torch-geometric"
      ],
      "metadata": {
        "id": "QOnyXI8HoI9d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# #@title\n",
        "# ! pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "lIySNFHLEpyB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# import torch\n",
        "\n",
        "# def format_pytorch_version(version):\n",
        "#     return version.split('+')[0]\n",
        "\n",
        "# def format_cuda_version(version):\n",
        "#     return 'cu' + version.replace('.', '')\n",
        "\n",
        "# TORCH_version = torch.__version__\n",
        "# TORCH = format_pytorch_version(TORCH_version)\n",
        "# CUDA_version = torch.version.cuda\n",
        "# CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "# !pip install torch-geometric\n",
        "# !pip install torch_geometric"
      ],
      "metadata": {
        "id": "mc1ptwP5xHwq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module Imports"
      ],
      "metadata": {
        "id": "riUQd_gG9PRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openbiolink.obl2021 import OBL2021Dataset, OBL2021Evaluator\n",
        "import torch\n",
        "from torch.nn import Module,\\\n",
        "                     ModuleList,\\\n",
        "                     Embedding,\\\n",
        "                     BatchNorm1d,\\\n",
        "                     ReLU,\\\n",
        "                     Linear,\\\n",
        "                     BCEWithLogitsLoss,\\\n",
        "                     MarginRankingLoss,\\\n",
        "                     CrossEntropyLoss,\\\n",
        "                     Dropout\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as PyG\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn.conv import RGCNConv\n",
        "# from torch_geometric.utils import to_networkx\n",
        "# import networkx as nx\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from typing import NoReturn\n",
        "import typing\n",
        "# import time\n",
        "from enum import Enum\n",
        "# from datetime import datetime\n",
        "# from collections import defaultdict"
      ],
      "metadata": {
        "id": "SYfjWfsh9UE5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "4885f47d-0e0f-439f-a419-8f7b26c73425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b56963fd0d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPyG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from torch_geometric.data.feature_store import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.7/dist-packages/torch_sparse/_spmm_cuda.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loading + Preprocessing"
      ],
      "metadata": {
        "id": "GTBimSEf9jkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QELCWgobmfUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3617de9b-6ee8-4fa4-eba1-351ce963adcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/ProjectI"
      ],
      "metadata": {
        "id": "mNiWRxONmfUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f52a8d-77b3-4e32-9c49-d2829883bbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ProjectI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = OBL2021Dataset()\n",
        "evaluator = OBL2021Evaluator()\n",
        "# kg = torch.cat((dataset.training, dataset.validation, dataset.testing), dim=0)\n",
        "entities = dataset.candidates\n",
        "train_set = dataset.training\n",
        "dev_set = dataset.validation\n",
        "test_set = dataset.testing"
      ],
      "metadata": {
        "id": "0GiwxvQK9t-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0d7dd8-b6aa-448a-ea47-2e6f9a1dc4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset found in /content/drive/MyDrive/ProjectI/obl2021, omitting download...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.stats"
      ],
      "metadata": {
        "id": "PcUcz0z-Kaj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "995d7b06-1576-46c1-9b05-6d3d48c13f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Triples:     \\n     Train 4192002\\n     Valid 186301\\n     Test  180964\\n# Relations:   28\\n# Entities:    180992\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Rzo66rzzAdME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistMult"
      ],
      "metadata": {
        "id": "UZo0DJkiAhpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistMultModel(Module):\n",
        "  def __init__(self, num_entities: int, num_relations: int, embedding_size: int) -> NoReturn:\n",
        "    super(DistMultModel, self).__init__()\n",
        "    self.num_entities = num_entities\n",
        "    self.num_relations = num_relations\n",
        "    self.embedding_size = embedding_size\n",
        "    self.entity_encoder = Embedding(self.num_entities, self.embedding_size)\n",
        "    self.relation_encoder = Embedding(self.num_relations, self.embedding_size)\n",
        "\n",
        "  def _encode_triplets(self, triplets: torch.Tensor) -> tuple:\n",
        "    h_heads = self.entity_encoder(triplets[:, 0])\n",
        "    h_tails = self.entity_encoder(triplets[:, 2])\n",
        "    h_relations = self.relation_encoder(triplets[:, 1])\n",
        "    return h_heads, h_relations, h_tails\n",
        "\n",
        "  def forward(self, data_triplets: tuple((torch.Tensor, torch.Tensor))) -> tuple:\n",
        "    p_triplets, n_triplets = data_triplets\n",
        "    p_heads, p_relations, p_tails = self._encode_triplets(p_triplets)\n",
        "    n_heads, n_relations, n_tails = self._encode_triplets(n_triplets)\n",
        "\n",
        "    p_scores = (p_heads * p_relations * p_tails).sum(-1)\n",
        "    n_scores = (n_heads * n_relations * n_tails).sum(-1)\n",
        "\n",
        "    return p_scores, n_scores"
      ],
      "metadata": {
        "id": "g8UwcJ7lAkCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESCAL"
      ],
      "metadata": {
        "id": "MjptK0FbKAxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RESCALModel(Module):\n",
        "  def __init__(self, num_entities: int, num_relations: int, embedding_size: int) -> NoReturn:\n",
        "    super(RESCALModel, self).__init__()\n",
        "    self.num_entities = num_entities\n",
        "    self.num_relations = num_relations\n",
        "    self.embedding_size = embedding_size\n",
        "    self.entity_encoder = Embedding(self.num_entities, self.embedding_size)\n",
        "    self.relation_encoder = Embedding(self.num_relations, self.embedding_size * self.embedding_size)\n",
        "\n",
        "  def _encode_triplets(self, triplets: torch.Tensor) -> tuple:\n",
        "    h_heads = self.entity_encoder(triplets[:, 0])\n",
        "    h_tails = self.entity_encoder(triplets[:, 2])\n",
        "    h_relations = self.relation_encoder(triplets[:, 1]).reshape(-1, self.embedding_size, self.embedding_size)\n",
        "    return h_heads, h_relations, h_tails\n",
        "\n",
        "  def forward(self, data_triplets: tuple((torch.Tensor, torch.Tensor))) -> tuple:\n",
        "    p_triplets, n_triplets = data_triplets\n",
        "    p_heads, p_relations, p_tails = self._encode_triplets(p_triplets)\n",
        "    n_heads, n_relations, n_tails = self._encode_triplets(n_triplets)\n",
        "\n",
        "    n_scores = torch.matmul(\n",
        "        torch.matmul(n_heads.reshape(-1, 1, self.embedding_size), n_relations),\n",
        "        n_tails.reshape(-1, self.embedding_size, 1)\n",
        "    )\n",
        "\n",
        "    p_scores = torch.matmul(\n",
        "        torch.matmul(p_heads.reshape(-1, 1, self.embedding_size), p_relations),\n",
        "        p_tails.reshape(-1, self.embedding_size, 1)\n",
        "    )\n",
        "\n",
        "    return p_scores.reshape(-1), n_scores.reshape(-1)"
      ],
      "metadata": {
        "id": "XO4i3tLkKDDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hinge Loss"
      ],
      "metadata": {
        "id": "Do6jyX4VOIMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HingeLoss(Module):\n",
        "  def __init__(self, margin):\n",
        "    super(HingeLoss, self).__init__()\n",
        "    self.margin = margin\n",
        "    self.loss_fn = MarginRankingLoss(self.margin)\n",
        "\n",
        "  def forward(self, p_scores: torch.Tensor, n_scores: torch.Tensor) -> torch.Tensor:\n",
        "    positive = p_scores.repeat(n_scores.shape)\n",
        "    negative = n_scores.repeat(p_scores.shape)\n",
        "\n",
        "    return self.loss_fn(positive, negative, torch.ones(n_scores.shape[0] * p_scores.shape[0]).to(positive.device))\n"
      ],
      "metadata": {
        "id": "v3eP3OgBOQbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Neural Network"
      ],
      "metadata": {
        "id": "dpqPM4awUBAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(Module):\n",
        "  def __init__(self, num_entities: int, num_relations: int, embedding_dim: int, conv_dims: tuple, dropout, num_bases=None, num_blocks=None):\n",
        "    super(GNN, self).__init__()\n",
        "    self.num_entities = num_entities\n",
        "    self.num_relations = num_relations\n",
        "    self.embedding_dim = embedding_dim \n",
        "    self.conv_channels = [self.embedding_dim] + list(conv_dims)\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.shallow_embedding = Embedding(self.num_entities, self.embedding_dim)\n",
        "    self.relation_embedding = Embedding(self.num_relations, self.conv_channels[-1])\n",
        "    self.emb_bn = BatchNorm1d(self.embedding_dim)\n",
        "    self.conv = ModuleList([\n",
        "                RGCNConv(dim, self.conv_channels[index + 1], num_relations=self.num_relations, num_bases=num_bases, num_blocks=num_blocks)\n",
        "                for index, dim in enumerate(self.conv_channels[:-1])\n",
        "    ])\n",
        "    self.conv_bn = ModuleList([\n",
        "                        BatchNorm1d(dim)\n",
        "                        for dim in self.conv_channels[1:]\n",
        "    ])\n",
        "    self.activation = ReLU()\n",
        "    self.dropout = Dropout(p=self.dropout)\n",
        "\n",
        "  def _node_encoder(self, data: Data):\n",
        "    x = data.x\n",
        "    x = self.shallow_embedding(x)\n",
        "    x = self.emb_bn(x)\n",
        "    x = self.dropout(x)\n",
        "    for conv, conv_bn in zip(self.conv, self.conv_bn):\n",
        "      x = conv(x, data.edge_index, data.edge_type) \n",
        "      x = conv_bn(x)\n",
        "      x = self.activation(x)\n",
        "      x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "  def _triplet_decoder(self, encoded, triplets):\n",
        "    heads = encoded[triplets[:, 0]]\n",
        "    relations = self.relation_embedding(triplets[:, 1])\n",
        "\n",
        "    #relations = self.dropout(relations)\n",
        "\n",
        "    tails = encoded[triplets[:, 2]]\n",
        "    return (heads * relations * tails).sum(-1)\n",
        "  \n",
        "  def forward(self, data: Data):\n",
        "    nodes_h = self._node_encoder(data)\n",
        "    ## decoding part of the auto-encoder\n",
        "    p_scores = self._triplet_decoder(nodes_h, data.positive)\n",
        "    n_scores = self._triplet_decoder(nodes_h, data.negative)\n",
        "    return p_scores, n_scores\n"
      ],
      "metadata": {
        "id": "s8qaQzPVZMDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Miner"
      ],
      "metadata": {
        "id": "pALKUNIP_lMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphMiner():\n",
        "  def __init__(self, dataset, device: str, cache_size: int=None, replacement_size: int=None) -> NoReturn:\n",
        "    self.device = device\n",
        "    self.dataset = dataset\n",
        "    self.training = self.dataset.training.to(self.device)\n",
        "    self.validation = self.dataset.validation.to(self.device)\n",
        "    self.testing = self.dataset.testing.to(self.device)\n",
        "    self.candidates = self.dataset.candidates.to(self.device)\n",
        "    if cache_size and replacement_size:\n",
        "      self.cache_size = cache_size\n",
        "      self.replacement_size = replacement_size\n",
        "      self.head_cache = (torch.rand(dataset.num_relations, entities.shape[0], cache_size) * entities.shape[0] - 1).round().long().to(self.device)\n",
        "      self.tail_cache = (torch.rand(entities.shape[0], dataset.num_relations, cache_size) * entities.shape[0] - 1).round().long().to(self.device)\n",
        "\n",
        "    self.num_entities = self.dataset.num_entities\n",
        "    self.num_relations = self.dataset.num_relations\n",
        "\n",
        "  def pyg_data(self, index, batch_size, negative_sampler):\n",
        "    indcs = torch.ones_like(self.training[:, 0], dtype=torch.bool)\n",
        "    not_include = torch.arange(index, index + batch_size, device=self.device)\n",
        "    indcs[not_include] = False\n",
        "\n",
        "    edge_index = self.training[indcs][:, (0, 2)].t().contiguous()\n",
        "    edge_type = self.training[indcs][:, 1].t().contiguous()\n",
        "    positive = self.training[index: index + batch_size, :]\n",
        "    negative = negative_sampler(positive)\n",
        "\n",
        "    return Data(\n",
        "        x=self.candidates,\n",
        "        edge_index=edge_index,\n",
        "        edge_type=edge_type,\n",
        "        positive=positive,\n",
        "        negative=negative\n",
        "    )\n",
        "\n",
        "  def uniform_negative_sample(self, batch):\n",
        "    batch_size = batch.shape[0]\n",
        "\n",
        "    head_relation = batch[:, (0, 1)].repeat(self.candidates.shape[0], 1)\n",
        "    relation_tail = batch[:, (1, 2)].repeat(self.candidates.shape[0], 1)\n",
        "\n",
        "    entities = self.candidates.reshape(-1, 1).repeat(batch_size, 1)\n",
        "    entities = torch.cat(torch.split(entities, self.candidates.shape[0]), -1).reshape(-1, 1)\n",
        "\n",
        "    corrupted_head_triplets = torch.cat(\n",
        "        (entities, relation_tail), \n",
        "        -1\n",
        "    )\n",
        "    corrupted_tail_triplets = torch.cat(\n",
        "        (head_relation, entities), \n",
        "        -1\n",
        "    )\n",
        "\n",
        "    negative_triplets = torch.cat((corrupted_head_triplets, corrupted_tail_triplets))\n",
        "    negative_triplets = torch.cat(torch.split(negative_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "    indcs = (torch.rand(batch_size) * (self.candidates.shape[0] * 2 - 1)).round().long().to(self.device)\n",
        "    splits = torch.split(negative_triplets.reshape(-1, 3), self.candidates.shape[0] * 2)\n",
        "    splits = torch.stack(splits).reshape(-1, 3)\n",
        "\n",
        "    indcs += torch.arange(batch_size).to(self.device) * (2 * self.candidates.shape[0])\n",
        "    negative_samples = splits[indcs]\n",
        "    return negative_samples\n",
        "\n",
        "  def cache_negative_sample(self, batch):\n",
        "    batch_size = batch.shape[0]\n",
        "    heads = batch[:, 0]\n",
        "    tails = batch[:, 2]\n",
        "    relations = batch[:, 1]\n",
        "\n",
        "    candidate_heads = self.head_cache[relations, tails]\n",
        "    candidate_tails = self.tail_cache[heads, relations]\n",
        "\n",
        "    corrupt_head_indcs = (torch.rand(batch_size) * (self.cache_size - 1)).round().long().to(self.device) # 1 for each item in the batch?\n",
        "    corrupt_tail_indcs = (torch.rand(batch_size) * (self.cache_size - 1)).round().long().to(self.device) # 1 for each item in the batch?\n",
        "\n",
        "    corrupt_head_indcs += torch.arange(batch_size).to(self.device) * self.cache_size\n",
        "    corrupt_heads = candidate_heads.reshape(-1)[corrupt_head_indcs]\n",
        "\n",
        "    corrupt_tail_indcs += torch.arange(batch_size).to(self.device) * self.cache_size\n",
        "    corrupt_tails = candidate_tails.reshape(-1)[corrupt_tail_indcs]\n",
        "\n",
        "    corrupted_head_triplets = torch.cat((corrupt_heads.reshape(-1, 1), batch[:, (1, 2)]), -1)\n",
        "    corrupted_tail_triplets = torch.cat((batch[:, (0, 1)], corrupt_tails.reshape(-1, 1)), -1)\n",
        "    negative_triplets = torch.cat((corrupted_head_triplets, corrupted_tail_triplets), 0)\n",
        "\n",
        "    negative_triplets = torch.cat(torch.split(negative_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "    indcs = torch.rand(batch_size).round().long().to(self.device)\n",
        "    splits = torch.split(negative_triplets.reshape(-1, 3), 1 * 2)\n",
        "\n",
        "    splits = torch.stack(splits).reshape(-1, 3)\n",
        "    indcs += torch.arange(batch_size).to(self.device) * (1 * 2)\n",
        "    negative_samples = splits[indcs]\n",
        "    # self.update_caches(batch, scoring_function)\n",
        "    return negative_samples\n",
        "\n",
        "  def _update_cache(self, batch, scoring_function, head_or_tail, **kwargs):\n",
        "    if kwargs.get('gnn'):\n",
        "      data = batch\n",
        "      batch = data.positive\n",
        "    batch_size = batch.shape[0]\n",
        "    heads = batch[:, 0]\n",
        "    tails = batch[:, 2]\n",
        "    relations = batch[:, 1]\n",
        "\n",
        "    replacement_candidates = (\n",
        "          torch.rand(\n",
        "              batch_size * self.replacement_size\n",
        "          ) * (self.candidates.shape[0] - 1)).round().long().reshape(batch_size, self.replacement_size).to(self.device)\n",
        "\n",
        "    if head_or_tail == 'head':\n",
        "      current_entities = self.head_cache[relations, tails]\n",
        "    elif head_or_tail == 'tail':\n",
        "      current_entities = self.tail_cache[heads, relations]\n",
        "\n",
        "    entities_pool = torch.cat((current_entities, replacement_candidates), -1)\n",
        "    if head_or_tail == 'head':\n",
        "      partial_triplets = batch[:, 1:]\n",
        "    elif head_or_tail == 'tail':\n",
        "      partial_triplets = batch[:, :2]\n",
        "    partial_triplets = partial_triplets.repeat(entities_pool.shape[1], 1)\n",
        "\n",
        "    partial_triplets = torch.cat(torch.split(partial_triplets, batch_size), -1).reshape(batch_size, -1, 2).reshape(-1, 2)\n",
        "\n",
        "    if head_or_tail == 'head':\n",
        "      pool = torch.cat((entities_pool.reshape(-1, 1), partial_triplets), -1)\n",
        "    elif head_or_tail == 'tail':\n",
        "      pool = torch.cat((partial_triplets, entities_pool.reshape(-1, 1)), -1)\n",
        "\n",
        "    dummy = torch.ones(1, 3).long().to(self.device)\n",
        "    input = (pool, dummy)\n",
        "    if kwargs.get('gnn'):\n",
        "      input = Data(\n",
        "              edge_index=data.edge_index,\n",
        "               edge_type=data.edge_type,\n",
        "               x=data.x,\n",
        "               positive=pool,\n",
        "               negative=dummy\n",
        "          )\n",
        "      \n",
        "    scoring_function.eval()\n",
        "    with torch.no_grad():\n",
        "      scores = scoring_function(input)[0]\n",
        "    \n",
        "    fitness = F.softmax(scores, 0)\n",
        "    split = torch.stack(torch.split(fitness, self.replacement_size + self.cache_size))\n",
        "\n",
        "    next_entities_indcs = split.multinomial(num_samples=self.cache_size)\n",
        "    if head_or_tail == 'head':\n",
        "      self.head_cache[relations, tails] = torch.take(entities_pool, next_entities_indcs)\n",
        "    elif head_or_tail == 'tail':\n",
        "      self.tail_cache[heads, relations] = torch.take(entities_pool, next_entities_indcs)\n",
        "\n",
        "  def update_caches(self, batch, scoring_function, **kwargs):\n",
        "    self._update_cache(batch, scoring_function, 'head', **kwargs)\n",
        "    self._update_cache(batch, scoring_function, 'tail', **kwargs)\n",
        "\n",
        "  def train(self, model: Module, data, optimizer: torch.optim, loss_fn, update_cache=False) -> tuple((torch.float, torch.Tensor, torch.Tensor)):\n",
        "    optimizer.zero_grad()\n",
        "    model.train()\n",
        "\n",
        "    positive, negative = model(data) # forward pass\n",
        "\n",
        "    loss = loss_fn(positive, negative) # margin ranking loss\n",
        "\n",
        "    loss.backward() # back propagation\n",
        "    optimizer.step() # parameter updates\n",
        "    if update_cache:\n",
        "      self.update_caches(data, model, gnn=True)\n",
        "    return loss.item()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def _ranks(self, model, batch, **kwargs):\n",
        "    if kwargs.get('gnn'):\n",
        "      data = batch\n",
        "      batch = data.positive\n",
        "    batch_size = batch.shape[0]\n",
        "\n",
        "    head_relation = batch[:, (0, 1)].repeat(self.candidates.shape[0], 1)\n",
        "    relation_tail = batch[:, (1, 2)].repeat(self.candidates.shape[0], 1)\n",
        "\n",
        "    entities = self.candidates.reshape(-1, 1).repeat(batch_size, 1)\n",
        "    entities = torch.cat(torch.split(entities, self.candidates.shape[0]), -1).reshape(-1, 1)\n",
        "\n",
        "    corrupted_head_triplets = torch.cat(\n",
        "        (entities, relation_tail), \n",
        "        -1\n",
        "    )\n",
        "    corrupted_tail_triplets = torch.cat(\n",
        "        (head_relation, entities), \n",
        "        -1\n",
        "    )\n",
        "    corrupted_head_triplets = torch.cat(torch.split(corrupted_head_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "    corrupted_tail_triplets = torch.cat(torch.split(corrupted_tail_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "\n",
        "    heads = batch[:, 0]\n",
        "    tails = batch[:, 2]\n",
        "\n",
        "    dummy = torch.ones(1, 3).long().to(self.device)\n",
        "    input = corrupted_head_triplets.reshape(-1, 3)\n",
        "    if kwargs.get('gnn'):\n",
        "      input = Data(\n",
        "              edge_index=data.edge_index,\n",
        "               edge_type=data.edge_type,\n",
        "               x=data.x,\n",
        "               positive=input,\n",
        "               negative=dummy\n",
        "          )\n",
        "    else:\n",
        "      input = (input, dummy)\n",
        "    model.eval()\n",
        "    head_scores = model(\n",
        "        input\n",
        "    )[0].reshape(batch_size, -1)\n",
        "    head_ranks = (head_scores.gather(dim=1, index=heads.reshape(-1, 1)) <= head_scores).sum(-1) # Max Policy\n",
        "\n",
        "    input = corrupted_tail_triplets.reshape(-1, 3)\n",
        "    if kwargs.get('gnn'):\n",
        "      input = Data(\n",
        "              edge_index=data.edge_index,\n",
        "               edge_type=data.edge_type,\n",
        "               x=data.x,\n",
        "               positive=input,\n",
        "               negative=dummy\n",
        "          )\n",
        "    else:\n",
        "      input = (input, dummy)\n",
        "    tail_scores = model(\n",
        "        input\n",
        "    )[0].reshape(batch_size, -1)\n",
        "    tail_ranks = (tail_scores.gather(dim=1, index=tails.reshape(-1, 1)) <= tail_scores).sum(-1)\n",
        "\n",
        "    return head_ranks.reshape(-1), tail_ranks.reshape(-1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def MRR(self, model, set_type, batch_size, use_tqdm=False, **kwargs):\n",
        "    if set_type == 'validation':\n",
        "      eval_set = self.validation\n",
        "      if kwargs.get('gnn'):\n",
        "        message = self.training\n",
        "    elif set_type == 'testing':\n",
        "      eval_set = self.testing\n",
        "      if kwargs.get('gnn'):\n",
        "        message = torch.cat((self.training, self.validation), 0)\n",
        "    else:\n",
        "      raise ValueError('Wrong data split specified.')\n",
        "    \n",
        "    sum = 0\n",
        "    size = 0\n",
        "\n",
        "    wrapper = tqdm if use_tqdm else (lambda z: z)\n",
        "    for batch in wrapper(torch.split(eval_set, batch_size)):\n",
        "      if kwargs.get('gnn'):\n",
        "        batch = Data(\n",
        "            edge_index=message[:, (0, 2)].t().contiguous(),\n",
        "            edge_type=message[:, 1].t().contiguous(),\n",
        "            x=self.candidates,\n",
        "            positive=batch\n",
        "        )\n",
        "      head_ranks, tail_ranks = self._ranks(model, batch, **kwargs)\n",
        "      sum += (1 / head_ranks).sum() + (1 / tail_ranks).sum()\n",
        "      size += head_ranks.shape[0] + tail_ranks.shape[0]\n",
        "    return sum / size\n",
        "\n",
        "    \n",
        "      "
      ],
      "metadata": {
        "id": "DfkRLMsc_nHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "tNvebpdt_3KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN"
      ],
      "metadata": {
        "id": "eMM5TBb_B7Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gm = GraphMiner(dataset, device, 50, 50)\n",
        "gm.head_cache = torch.load('head.pt').to(device)\n",
        "gm.tail_cache = torch.load('tail.pt').to(device)\n",
        "# gnn = GNN(gm.num_entities, gm.num_relations, 32, (32, 16, 16), 0.4, num_bases=7).to(device)\n",
        "gnn = torch.load('gnn.pth').to(device)\n",
        "\n",
        "print(gnn)\n",
        "optimizer = Adam(gnn.parameters())\n",
        "margin = 1\n",
        "criterion = HingeLoss(margin=margin)\n",
        "batch_size = 256\n",
        "num_epochs = 10\n",
        "where_i = 0\n",
        "where_epoch = 0\n",
        "what_loss = 0"
      ],
      "metadata": {
        "id": "mS82r9UiprbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('where.txt') as fp:\n",
        "  args = fp.read().split(', ')\n",
        "  where_i = int(args[0])\n",
        "  where_epoch = int(args[1])\n",
        "  what_loss = float(args[2])\n",
        "print(where_i, where_epoch, what_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdnSLV9iAiP2",
        "outputId": "593ff8e4-181c-425c-91f9-371aa38a9404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128000 5 68.99426233023405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gm.training = gm.training[torch.randperm(gm.training.shape[0])]\n",
        "for epoch in range(where_epoch, num_epochs):\n",
        "  epoch_loss = what_loss\n",
        "\n",
        "  for i in tqdm(range(where_i, gm.training.shape[0], batch_size)):\n",
        "    if i + batch_size > gm.training.shape[0]:\n",
        "      continue\n",
        "    train_data = gm.pyg_data(i, batch_size, gm.cache_negative_sample)\n",
        "    batch_loss = gm.train(gnn, train_data, optimizer, criterion, True)\n",
        "    epoch_loss += batch_loss\n",
        "    if (i // batch_size) % 500 == 0:\n",
        "      with open('where.txt', 'w') as fp:\n",
        "        fp.write(f'{i}, {epoch}, {epoch_loss}')\n",
        "      torch.save(gnn, 'gnn.pth')\n",
        "      torch.save(gm.head_cache, 'head.pt')\n",
        "      torch.save(gm.tail_cache, 'tail.pt')\n",
        "\n",
        "  where_i = 0\n",
        "  what_loss = 0\n",
        "  print(f'Epoch {epoch}')\n",
        "  print(epoch_loss)  \n",
        "  # if epoch % 10 == 1:\n",
        "  #   print(f'Validation MRR: {gm.MRR(gnn, \"validation\", 16, True, gnn=True)}')"
      ],
      "metadata": {
        "id": "Bv1m6nZEuipx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "100%|██████████| 16376/16376 [2:17:45<00:00,  1.98it/s]\n",
        "Epoch 0\n",
        "4937.591060988605\n",
        "100%|██████████| 8376/8376 [1:10:37<00:00,  1.98it/s]\n",
        "Epoch 1\n",
        "3108.513555571437\n",
        "100%|██████████| 16376/16376 [2:17:27<00:00,  1.99it/s]\n",
        "Epoch 2\n",
        "2639.6009912267327\n",
        "100%|██████████| 14876/14876 [2:04:00<00:00,  2.00it/s]\n",
        "Epoch 3\n",
        "2390.9145727604628\n",
        "100%|██████████| 4376/4376 [36:47<00:00,  1.98it/s]\n",
        "Epoch 4\n",
        "2321.6044249273837"
      ],
      "metadata": {
        "id": "sBq6-4OLIKPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GraphMiner(dataset, 'cuda')\n",
        "gnn = torch.load('gnn.pth').to('cuda')\n",
        "print(f'Validation MRR: {gm.MRR(gnn, \"validation\", 64, True, gnn=True)}')"
      ],
      "metadata": {
        "id": "KXKUbSIacAUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xct08N5-At1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = [\n",
        "    4937.591060988605, \n",
        "    3108.513555571437, \n",
        "    2639.6009912267327, \n",
        "    2390.9145727604628, \n",
        "    2321.6044249273837\n",
        "]\n",
        "loss = np.array(loss) / (train_set.shape[0] // batch_size)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkXIkSPeAymT",
        "outputId": "6c809e01-c664-4cfc-b298-5f4d4acaab71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.30153228, 0.18983289, 0.16119701, 0.14601005, 0.14177737])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1, 6), loss)\n",
        "plt.xticks(np.arange(1, 6))\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean batch loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "5aMjulpaB2LL",
        "outputId": "e0b5b728-8572-4e56-e5ed-29fc44a762f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.shape[0] / batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rpGLv0IBVzW",
        "outputId": "6ec449cf-0518-44d2-f2f6-84b4f7cb1d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16375.0078125"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistMult"
      ],
      "metadata": {
        "id": "lwbBkhMFCCV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gm = GraphMiner(dataset, 'cpu', 50, 50)\n",
        "dm = DistMultModel(gm.num_entities, gm.num_relations, 256)\n",
        "optimizer = Adam(dm.parameters(), weight_decay=3e-8)\n",
        "margin = 1\n",
        "criterion = HingeLoss(margin=margin)\n",
        "batch_size = 64\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "7ZKLZPeX5h6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = gm.training[:64]\n",
        "gm.cache_negative_sample(batch, dm)"
      ],
      "metadata": {
        "id": "wi0VeyhJVP43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  train_batches = torch.split(gm.training, batch_size)\n",
        "  epoch_loss = 0\n",
        "  for batch in tqdm(train_batches):\n",
        "    negative = gm.cache_negative_sample(batch, dm)\n",
        "    epoch_loss += gm.train(dm, (batch, negative), optimizer, criterion)\n",
        "  print(f'Epoch {epoch}')\n",
        "  print(epoch_loss)  \n",
        "  if epoch % 10 == 0:\n",
        "    print(f'Validation MRR: {gm.MRR(dm, \"validation\", 64, True)}')"
      ],
      "metadata": {
        "id": "WLaXCuarUhdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESCAL"
      ],
      "metadata": {
        "id": "RnGIUSfqCFF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rscl = RESCALModel(dataset.num_entities, gm.num_relations, 4).to('cuda')\n",
        "optm = Adam(rscl.parameters(), 1e-3)\n",
        "\n",
        "p = train_set[:100].to('cuda')\n",
        "n = gm.cache_negative_sample(p)\n",
        "# rscl((p, n))[0]"
      ],
      "metadata": {
        "id": "lRn0pwKSLAqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  rscl.train()\n",
        "  p = train_set[:100].to('cuda')\n",
        "  n = gm.cache_negative_sample(p)\n",
        "  p_s, n_s = rscl((p, n))\n",
        "  loss = gm.margin_ranking_loss(p_s, n_s)\n",
        "  loss.backward()\n",
        "  optm.step()\n",
        "  print(loss.item())"
      ],
      "metadata": {
        "id": "nJqUJ4QeMQ5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm."
      ],
      "metadata": {
        "id": "HGV4xn9TpZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One by One Graph Miner"
      ],
      "metadata": {
        "id": "h6dlbbZlkTcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneGM():\n",
        "  def __init__(self, dataset, device: str, cache_size: int=None, replacement_size: int=None) -> NoReturn:\n",
        "    self.device = device\n",
        "    self.dataset = dataset\n",
        "    self.training = self.dataset.training.to(self.device)\n",
        "    self.validation = self.dataset.validation.to(self.device)\n",
        "    self.testing = self.dataset.testing.to(self.device)\n",
        "    self.candidates = self.dataset.candidates.to(self.device)\n",
        "    if cache_size and replacement_size:\n",
        "      self.cache_size = cache_size\n",
        "      self.replacement_size = replacement_size\n",
        "      self.head_cache = (torch.rand(dataset.num_relations, entities.shape[0], cache_size) * entities.shape[0] - 1).round().long().to(self.device)\n",
        "      self.tail_cache = (torch.rand(entities.shape[0], dataset.num_relations, cache_size) * entities.shape[0] - 1).round().long().to(self.device)\n",
        "\n",
        "    self.num_entities = self.dataset.num_entities\n",
        "    self.num_relations = self.dataset.num_relations\n",
        "\n",
        "  def pyg_data(self, index, batch_size, negative_sampler):\n",
        "    indcs = torch.ones_like(self.training[:, 0], dtype=torch.bool)\n",
        "    not_include = torch.arange(index, index + batch_size, device=self.device)\n",
        "    indcs[not_include] = False\n",
        "\n",
        "    edge_index = self.training[indcs][:, (0, 2)].t().contiguous()\n",
        "    edge_type = self.training[indcs][:, 1].t().contiguous()\n",
        "    positive = self.training[index: index + batch_size, :]\n",
        "    negative = negative_sampler(positive)\n",
        "\n",
        "    return Data(\n",
        "        x=self.candidates,\n",
        "        edge_index=edge_index,\n",
        "        edge_type=edge_type,\n",
        "        positive=positive,\n",
        "        negative=negative\n",
        "    )\n",
        "\n",
        "  def uniform_negative_sample(self, triplet, check=False):\n",
        "    okay = False\n",
        "    while not okay:\n",
        "      h, r, t = triplet\n",
        "      index = torch.round(torch.rand(1) * (self.candidates.shape[0] * 2)).long()\n",
        "      if index < self.candidates.shape[0]:\n",
        "        # corrupt head\n",
        "        negative_triplet = torch.tensor([self.candidates[index], r, t]).to(self.device).reshape(1, 3)\n",
        "      else:\n",
        "        # corrupt tail\n",
        "        index -= self.candidates.shape[0]\n",
        "        negative_triplet = torch.tensor([h, r, self.candidates[index]]).to(self.device).reshape(1, 3)\n",
        "      okay = (not check) or (not ((negative_triplet == self.training).sum(-1) == 3).sum())\n",
        "      return negative_triplet\n",
        "\n",
        "  def cache_negative_sample(self, triplet, check=False):\n",
        "    okay = False\n",
        "    h, r, t = triplet\n",
        "    index = torch.round(torch.rand(1) * self.cache_size - 1).long()\n",
        "    h_bar = self.head_cache[r][t][index]\n",
        "    index = torch.round(torch.rand(1) * self.cache_size - 1).long()\n",
        "    t_bar = self.tail_cache[h][r][index]\n",
        "    if torch.rand(1) < 0.5:\n",
        "      negative_triplet = torch.tensor([h_bar, r, t]).reshape(1, 3).to(self.device)\n",
        "    else:\n",
        "      negative_triplet = torch.tensor([h, r, t_bar]).reshape(1, 3).to(self.device)\n",
        "\n",
        "  def _update_cache(self, triplet, scoring_function, head_or_tail, **kwargs):\n",
        "    data = triplet\n",
        "    triplet = data.positive\n",
        "    h, r, t = triplet\n",
        "\n",
        "    new_candidates = torch.round(torch.rand(self.replacement_size) * (self.candidates.shape[0] - 1)).long()\n",
        "    if head_or_tail == 'head':\n",
        "      all_entities = torch.cat((self.head_cache[r][t], new_candidates)).reshape(-1, 1)\n",
        "    elif head_or_tail == 'tail':\n",
        "      all_entities = torch.cat((self.tail_cache[h][r], new_candidates)).reshape(-1, 1)\n",
        "    if head_or_tail == 'head':\n",
        "      partial = torch.tensor([r, t]).reshape(1, 2).repeat(self.cache_size + self.replacement_size, 1)\n",
        "      all_triplets = torch.cat((all_entities, partial), -1)\n",
        "    elif head_or_tail == 'tail':\n",
        "      partial = torch.tensor([h, r]).reshape(1, 2).repeat(cache_size + replacement_size, 1)\n",
        "      all_triplets = torch.cat((partial, all_entities), -1)\n",
        "    dummy = torch.ones(1, 3).long()\n",
        "    input = (all_triplets, dummy)\n",
        "    if kwargs.get('gnn'):\n",
        "      indcs = torch.ones(self.training.shape[0], dtype=torch.bool)\n",
        "      indcs[i] = False\n",
        "      input = Data(\n",
        "            x=data.x,\n",
        "            edge_index=data.edge_index,\n",
        "            edge_type=data.edge_type,\n",
        "            positive=all_triplets,\n",
        "            negative=dummy\n",
        "        )\n",
        "      \n",
        "    scoring_function.eval()\n",
        "    with torch.no_grad():\n",
        "      scores = scoring_function(input)[0]\n",
        "    fitness = F.softmax(scores, -1)\n",
        "    if head_or_tail == 'head':\n",
        "      self.head_cache[r][t] = all_entities.reshape(-1)[fitness.multinomial(num_samples=self.cache_size, replacement=False)]\n",
        "    elif head_or_tail == 'tail':\n",
        "      self.tail_cache[h][r] = all_entities.reshape(-1)[fitness.multinomial(num_samples=self.cache_size, replacement=False)]\n",
        "\n",
        "  def update_caches(self, batch, scoring_function, **kwargs):\n",
        "    self._update_cache(batch, scoring_function, 'head', **kwargs)\n",
        "    self._update_cache(batch, scoring_function, 'tail', **kwargs)\n",
        "\n",
        "  def train(self, model: Module, data, optimizer: torch.optim, loss_fn, update_cache=False) -> tuple((torch.float, torch.Tensor, torch.Tensor)):\n",
        "    optimizer.zero_grad()\n",
        "    model.train()\n",
        "\n",
        "    positive, negative = model(data) # forward pass\n",
        "\n",
        "    loss = loss_fn(positive, negative, torch.ones(1).to(self.device)) # margin ranking loss\n",
        "\n",
        "    loss.backward() # back propagation\n",
        "    optimizer.step() # parameter updates\n",
        "    if update_cache:\n",
        "      self.update_caches(data, model, gnn=True)\n",
        "    return loss.item()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def _ranks(self, model, batch):\n",
        "    batch_size = batch.shape[0]\n",
        "\n",
        "    head_relation = batch[:, (0, 1)].repeat(self.candidates.shape[0], 1)\n",
        "    relation_tail = batch[:, (1, 2)].repeat(self.candidates.shape[0], 1)\n",
        "\n",
        "    entities = self.candidates.reshape(-1, 1).repeat(batch_size, 1)\n",
        "    entities = torch.cat(torch.split(entities, self.candidates.shape[0]), -1).reshape(-1, 1)\n",
        "\n",
        "    corrupted_head_triplets = torch.cat(\n",
        "        (entities, relation_tail), \n",
        "        -1\n",
        "    )\n",
        "    corrupted_tail_triplets = torch.cat(\n",
        "        (head_relation, entities), \n",
        "        -1\n",
        "    )\n",
        "    corrupted_head_triplets = torch.cat(torch.split(corrupted_head_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "    corrupted_tail_triplets = torch.cat(torch.split(corrupted_tail_triplets, batch_size), -1).reshape(batch_size, -1, 3)\n",
        "\n",
        "    heads = batch[:, 0]\n",
        "    tails = batch[:, 2]\n",
        "\n",
        "    dummy = torch.ones(1, 3).long().to(self.device)\n",
        "\n",
        "    model.eval()\n",
        "    head_scores = model(\n",
        "        (dummy, corrupted_head_triplets.reshape(-1, 3))\n",
        "    )[1].reshape(batch_size, -1)\n",
        "    head_ranks = (head_scores.gather(dim=1, index=heads.reshape(-1, 1)) <= head_scores).sum(-1) # Max Policy\n",
        "\n",
        "    tail_scores = model(\n",
        "        (dummy, corrupted_tail_triplets.reshape(-1, 3))\n",
        "    )[1].reshape(batch_size, -1)\n",
        "    tail_ranks = (tail_scores.gather(dim=1, index=tails.reshape(-1, 1)) <= tail_scores).sum(-1)\n",
        "\n",
        "    return head_ranks.reshape(-1), tail_ranks.reshape(-1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def MRR(self, model, set_type, batch_size, use_tqdm=False):\n",
        "    if set_type == 'validation':\n",
        "      eval_set = self.validation\n",
        "    elif set_type == 'testing':\n",
        "      eval_set = self.testing\n",
        "    else:\n",
        "      raise ValueError('Wrong data split specified.')\n",
        "    \n",
        "    sum = 0\n",
        "    size = 0\n",
        "    wrapper = tqdm if use_tqdm else (lambda z: z)\n",
        "    for batch in wrapper(torch.split(eval_set, batch_size)):\n",
        "      head_ranks, tail_ranks = self._ranks(model, batch)\n",
        "      sum += (1 / head_ranks).sum() + (1 / tail_ranks).sum()\n",
        "      size += head_ranks.shape[0] + tail_ranks.shape[0]\n",
        "    return sum / size\n",
        "      "
      ],
      "metadata": {
        "id": "jkiv69KokYfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnn = GNN(dataset.num_entities, dataset.num_relations, 32, (32, 32, 32), 0.4)\n",
        "optimizer = Adam(gnn.parameters())\n",
        "margin = 1\n",
        "criterion = MarginRankingLoss(margin)"
      ],
      "metadata": {
        "id": "y-vQCEkSsmbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one(model, data, criterion, optimizer):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  p, n = model(data)\n",
        "  loss = criterion(p, n, torch.ones(1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "gZfl9l98rC2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache_size = 10\n",
        "# head_cache = torch.rand(cache_size) * entities.shape[0]\n",
        "head_cache = (torch.rand(dataset.num_relations, entities.shape[0], cache_size) * entities.shape[0] - 1).round().long()\n",
        "tail_cache = (torch.rand(entities.shape[0], dataset.num_relations, cache_size) * entities.shape[0] - 1).round().long()\n",
        "\n",
        "# train_set\n",
        "# h, r, t = triplet\n"
      ],
      "metadata": {
        "id": "dP4xujppu6qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   for triplet in tqdm(train_set):\n",
        "#     h, r, t = triplet\n",
        "#     index = torch.round(torch.rand(1) * cache_size - 1).long()\n",
        "#     h_bar = head_cache[r][t][index]\n",
        "#     index = torch.round(torch.rand(1) * cache_size - 1).long()\n",
        "#     t_bar = tail_cache[h][r][index]\n",
        "#     if torch.rand(1) < 0.5:\n",
        "#       negative_triplet = torch.tensor([h_bar, r, t]).reshape(1, 3)\n",
        "#     else:\n",
        "#       negative_triplet = torch.tensor([h, r, t_bar]).reshape(1, 3)\n",
        "# except KeyboardInterrupt:\n",
        "#   pass\n",
        "h, r, t = triplet\n",
        "head_or_tail = 'tail'\n",
        "i = 0\n",
        "replacement_size = 10\n",
        "\n",
        "new_candidates = torch.round(torch.rand(replacement_size) * (entities.shape[0] - 1)).long()\n",
        "if head_or_tail == 'head':\n",
        "  all_entities = torch.cat((head_cache[r][t], new_candidates)).reshape(-1, 1)\n",
        "elif head_or_tail == 'tail':\n",
        "  all_entities = torch.cat((tail_cache[h][r], new_candidates)).reshape(-1, 1)\n",
        "if head_or_tail == 'head':\n",
        "  partial = torch.tensor([r, t]).reshape(1, 2).repeat(cache_size + replacement_size, 1)\n",
        "  all_triplets = torch.cat((all_entities, partial), -1)\n",
        "elif head_or_tail == 'tail':\n",
        "  partial = torch.tensor([h, r]).reshape(1, 2).repeat(cache_size + replacement_size, 1)\n",
        "  all_triplets = torch.cat((partial, all_entities), -1)\n",
        "dummy = torch.ones(1, 3).long()\n",
        "input = (all_triplets, dummy)\n",
        "kwargs = {}\n",
        "if True or kwargs.get('gnn'):\n",
        "  indcs = torch.ones(train_set.shape[0], dtype=torch.bool)\n",
        "  indcs[i] = False\n",
        "  input = Data(\n",
        "        x=entities,\n",
        "        edge_index=train_set[indcs][:, (0, 2)].t().contiguous(),\n",
        "        edge_type=train_set[indcs][:, 1].t().contiguous(),\n",
        "        positive=all_triplets,\n",
        "        negative=dummy\n",
        "    )\n",
        "  \n",
        "gnn.eval()\n",
        "with torch.no_grad():\n",
        "  scores = gnn(input)[0]\n",
        "fitness = F.softmax(scores, -1)\n",
        "if head_or_tail == 'head':\n",
        "  head_cache[r][t] = all_entities.reshape(-1)[fitness.multinomial(num_samples=cache_size, replacement=False)]\n",
        "elif head_or_tail == 'tail':\n",
        "  tail_cache[h][r] = all_entities.reshape(-1)[fitness.multinomial(num_samples=cache_size, replacement=False)]"
      ],
      "metadata": {
        "id": "xwY0JusewGEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGWR4cy35fdV",
        "outputId": "51deed8d-0ee9-43bb-e2f2-edda667b8a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  for i, triplet in tqdm(enumerate(train_set)):\n",
        "    okay = False\n",
        "    while not okay:\n",
        "      h, r, t = triplet\n",
        "      index = torch.round(torch.rand(1) * (entities.shape[0] * 2)).long()\n",
        "      if index < entities.shape[0]:\n",
        "        # corrupt head\n",
        "        negative_triplet = torch.tensor([entities[index], r, t]).reshape(1, 3)\n",
        "      else:\n",
        "        # corrupt tail\n",
        "        index -= entities.shape[0]\n",
        "        negative_triplet = torch.tensor([h, r, entities[index]])\n",
        "      # okay = not ((negative_triplet == train_set).sum(-1) == 3).sum()\n",
        "      okay = True\n",
        "\n",
        "    indcs = torch.ones(train_set.shape[0], dtype=torch.bool)\n",
        "    indcs[i] = False\n",
        "    train_data = Data(\n",
        "        x=entities,\n",
        "        edge_index=train_set[indcs][:, (0, 2)].t().contiguous(),\n",
        "        edge_type=train_set[indcs][:, 1].t().contiguous(),\n",
        "        positive=triplet.reshape(1, 3),\n",
        "        negative=negative_triplet.reshape(1, 3)\n",
        "    )\n",
        "    print(train_one(gnn, train_data, criterion, optimizer))\n",
        "    # break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oJ6cJcjknUJ",
        "outputId": "03bf8a4a-dbed-467f-9709-cac26ff0805a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:13, 13.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:25, 12.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4341979026794434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:38, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.070107460021973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:39, 13.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MarginRankingLoss()()"
      ],
      "metadata": {
        "id": "5kPA45AetGzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
      ],
      "metadata": {
        "id": "SG1fLGKFNO8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metric"
      ],
      "metadata": {
        "id": "3T3u_MuyCuDO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87ZALUBYec9",
        "cellView": "form"
      },
      "source": [
        "#@title find_start_and_end_indcs() PRIVATE-FUNCTION { form-width: \"15%\" }\n",
        "def find_start_and_end_indcs(entity, relation, entity_type, kg_sorted):\n",
        "  up = kg_sorted.shape[0]\n",
        "  down = 0\n",
        "  indx = kg_sorted.shape[0] // 2\n",
        "  found = False\n",
        "  while up - down > 1:\n",
        "    if kg_sorted[indx][entity_type].item() == entity:\n",
        "      found = True\n",
        "      break \n",
        "    elif kg_sorted[indx][entity_type].item() >= entity: \n",
        "      up = indx\n",
        "      indx = (up + down) // 2\n",
        "    else:\n",
        "      down = indx \n",
        "      indx = (up + down) // 2\n",
        "  if not found:\n",
        "    return None\n",
        "  while 1:\n",
        "    indx += 1\n",
        "    try:\n",
        "      if not kg_sorted[indx][entity_type].item() == entity:\n",
        "        indx -= 1\n",
        "        end_indx = indx\n",
        "        break\n",
        "    except:\n",
        "      end_indx = indx - 1\n",
        "      break\n",
        "  \n",
        "\n",
        "  while 1:\n",
        "    indx -= 1\n",
        "    try:\n",
        "      if not kg_sorted[indx][entity_type].item() == entity:\n",
        "        indx += 1\n",
        "        start_indx = indx\n",
        "        break\n",
        "    except:\n",
        "      start_indx = indx + 1\n",
        "      break\n",
        "  indcs = torch.arange(start_indx, end_indx + 1)\n",
        "  tns = kg_sorted[indcs]\n",
        "  srt_indcs = torch.sort(tns[:, 1])[1]\n",
        "\n",
        "  msk = tns[srt_indcs][:, 1] == relation\n",
        "  okay_indcs = torch.nonzero(msk).reshape(-1)\n",
        "  return indcs[srt_indcs[okay_indcs]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYPeOqPxkFtD"
      },
      "source": [
        "#@title get_psuedo_negative_entities() FUNCTIONS { form-width: \"15%\" }\n",
        "def get_psuedo_negative_entities(entity, relation, corrupt_at, kg_sorted):\n",
        "  if corrupt_at == Global.HEAD_INDEX.value:\n",
        "    entity_type = Global.TAIL_INDEX.value \n",
        "  elif corrupt_at == Global.TAIL_INDEX.value:\n",
        "    entity_type = Global.HEAD_INDEX.value\n",
        "\n",
        "  indcs = find_start_and_end_indcs(entity, relation, entity_type, kg_sorted=kg_sorted)\n",
        "  \n",
        "  if indcs is not None:\n",
        "    fact_triplets_entities = kg_sorted[indcs][:, corrupt_at]\n",
        "    features_copy = features.detach().clone()\n",
        "    features_copy[fact_triplets_entities] = -1\n",
        "    non_negative_mask = features_copy >= 0\n",
        "    ret = torch.nonzero(non_negative_mask).reshape(-1)\n",
        "    return ret\n",
        "  else:\n",
        "    return features.to(Global.DEVICE.value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcSjKWmGyYuj"
      },
      "source": [
        "#@title evaluation_rank() PRIVATE-FUNCTION { form-width: \"10%\" }\n",
        "@torch.no_grad()\n",
        "def evaluation_rank(model, eval_set, messaging_set):\n",
        "  h_pred_top10 = list()\n",
        "  t_pred_top10 = list()\n",
        "\n",
        "  for eval_triplet in tqdm(eval_set):\n",
        "    #use tqdm\n",
        "    head = eval_triplet[Global.HEAD_INDEX.value].item()\n",
        "    relation = eval_triplet[Global.RELATION_INDEX.value].item()\n",
        "    tail = eval_triplet[Global.TAIL_INDEX.value].item()\n",
        "\n",
        "    corrupted_tails = get_psuedo_negative_entities(\n",
        "        entity=head, \n",
        "        corrupt_at=Global.TAIL_INDEX.value, \n",
        "        kg_sorted=kg_sorted_heads\n",
        "    )\n",
        "    \n",
        "    num_psuedo_negative_triples = corrupted_tails.shape[0]\n",
        "    psuedo_negative_triplets_corrupted_tail = torch.vstack(\n",
        "        (\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * head,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * relation,\n",
        "            corrupted_tails\n",
        "        )\n",
        "    ).t().to(Global.DEVICE.value)\n",
        "\n",
        "    corrupted_heads = get_psuedo_negative_entities(\n",
        "        entity=tail, \n",
        "        corrupt_at=Global.HEAD_INDEX.value, \n",
        "        kg_sorted=kg_sorted_tails\n",
        "    )\n",
        "\n",
        "    num_psuedo_negative_triples = corrupted_heads.shape[0]\n",
        "    psuedo_negative_triplets_corrupted_head = torch.vstack(\n",
        "        (\n",
        "            corrupted_heads,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * relation,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * tail\n",
        "        )\n",
        "    ).t().to(Global.DEVICE.value)\n",
        "\n",
        "    # eval_triplet: (h, r, t)\n",
        "    # psuedo_negative_triplets_head: (h′, r, t) for all h′\n",
        "    # psuedo_negative_triplets_tail: (h, r, t′) for all t′\n",
        "\n",
        "    # train_set being the messaging graph, calculate the score for (h, r, t)\n",
        "    # train_set being the messaging graph, calculate the scores for all (h′, r, t)\n",
        "    # train_set being the messaging graph, calculate the scores for all (h, r, t′)\n",
        "\n",
        "    graph_data_for_object_tail = graph_data_maker(\n",
        "      messaging=messaging_set,\n",
        "      supervision=eval_triplet.reshape(1, 3),\n",
        "      negative_samples=psuedo_negative_triplets_corrupted_tail,\n",
        "      x=features.to(Global.DEVICE.value),\n",
        "      x_feature='one-hot'\n",
        "    )\n",
        "\n",
        "    graph_data_for_object_head = graph_data_maker(\n",
        "            messaging=messaging_set,\n",
        "            supervision=eval_triplet.reshape(1, 3),\n",
        "            negative_samples=psuedo_negative_triplets_corrupted_head,\n",
        "            x=features.to(Global.DEVICE.value),\n",
        "            x_feature='one-hot'\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    scores_for_object_tail = torch.cat(model(graph_data_for_object_tail))\n",
        "\n",
        "    sorted_by_scores_for_object_tail_indcs = torch.sort(scores_for_object_tail, descending=True)[1]\n",
        "\n",
        "    tail_objects = torch.cat((graph_data_for_object_tail.edge_index_supervision[1], graph_data_for_object_tail.edge_index_negative[1]))\n",
        "    top10_tails = tail_objects[sorted_by_scores_for_object_tail_indcs[:10]]\n",
        "    t_pred_top10.append(top10_heads)\n",
        "\n",
        "    model.eval()\n",
        "    scores_for_object_head = torch.cat(model(graph_data_for_object_head))\n",
        "\n",
        "    sorted_by_scores_for_object_head_indcs = torch.sort(scores_for_object_head, descending=True)[1]\n",
        "\n",
        "    head_objects = torch.cat((graph_data_for_object_head.edge_index_supervision[0], graph_data_for_object_head.edge_index_negative[0]))\n",
        "    top10_heads = head_objects[sorted_by_scores_for_object_head_indcs[:10]]\n",
        "    h_pred_top10.append(top10_tails)\n",
        "\n",
        "    # print('')\n",
        "    ############################################################################## 1\n",
        "    # if False:\n",
        "    #   hit_index = (\n",
        "    #             tail_objects[sorted_by_scores_for_object_head_indcs[:500]] == eval_triplet[2]\n",
        "    #           ).nonzero(as_tuple=True)[0] + 1\n",
        "    #   if hit_index.shape[0]:\n",
        "    #     print(f'Tail ranked at {hit_index.item()}')\n",
        "    #     print('-' * 30)\n",
        "    # ############################################################################### 2\n",
        "\n",
        "    # ############################################################################### 3\n",
        "    # hit_index = (\n",
        "    #             head_objects[sorted_by_scores_for_object_tail_indcs[:500]] == eval_triplet[0]\n",
        "    #           ).nonzero(as_tuple=True)[0] + 1\n",
        "    # if hit_index.shape[0]:\n",
        "    #   print(f'Head ranked at {hit_index.item()}')\n",
        "    #   print('-' * 30)\n",
        "    ############################################################################## 4\n",
        "    \n",
        "  model.train()\n",
        "  return torch.stack(h_pred_top10), torch.stack(t_pred_top10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzR8BWA0N628"
      },
      "source": [
        "#@title evaluate_hits_at_10(model, mode) FUNCTION{ form-width: \"15%\" }\n",
        "def evaluate_hits_at_10(model: GNN, mode:str) -> torch.float:\n",
        "  if mode == 'validation':\n",
        "    head_objects_top10, tail_objects_top10 = evaluation_rank(model, val_set, train_set)\n",
        "    return evaluator.eval(\n",
        "            head_objects_top10,\n",
        "            tail_objects_top10,\n",
        "            val_set,\n",
        "            False\n",
        "    )\n",
        "  elif mode == 'testing':\n",
        "    head_objects_top10, tail_objects_top10 = evaluation_rank(model, test_set, torch.cat((train_set, val_set), dim=0))\n",
        "    return evaluator.eval(\n",
        "            head_objects_top10,\n",
        "            tail_objects_top10,\n",
        "            test_set,\n",
        "            False\n",
        "          )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}